import feedparser
import requests
import time
import os
import json
import sys
import html
import re
from datetime import datetime, date
from dotenv import load_dotenv 
import pytz
from collections import defaultdict

load_dotenv()

# --- 1. é…ç½®è¯»å– ---
def load_config():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    config_path = os.path.join(project_root, 'config.json')
    
    if not os.path.exists(config_path):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶ {config_path}")
        sys.exit(1)
        
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)

BOT_TOKEN = os.environ.get("TG_BOT_TOKEN") 
CHAT_ID = os.environ.get("TG_CHAT_ID")

def get_rss_url(all_categories):
    unique_cats = list(set(all_categories))
    category_str = "+".join(unique_cats)
    return f"http://export.arxiv.org/rss/{category_str}"

def send_telegram_message(message):
    if not BOT_TOKEN or not CHAT_ID:
        print(">> [è·³è¿‡å‘é€] (æœªé…ç½® Token æˆ– Chat ID)")
        return

    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    payload = {
        "chat_id": CHAT_ID,
        "text": message,
        "parse_mode": "HTML",
        "disable_web_page_preview": True
    }
    try:
        response = requests.post(url, json=payload)
        response.raise_for_status()
    except Exception as e:
        print(f"!! å‘é€å¤±è´¥: {e}")

def is_today(entry_date_struct):
    entry_date = date(entry_date_struct.tm_year, entry_date_struct.tm_mon, entry_date_struct.tm_mday)
    today = datetime.utcnow().date()
    return entry_date == today

def get_paper_tags(entry):
    try:
        return [t['term'] for t in entry.tags]
    except (AttributeError, KeyError):
        return []

# --- å…³é”®è¯æ£€æŸ¥å‡½æ•° ---
def check_keywords(entry, keywords):
    """
    æ£€æŸ¥æ ‡é¢˜å’Œæ‘˜è¦æ˜¯å¦åŒ…å«å…³é”®è¯ã€‚
    è¿”å›åŒ¹é…åˆ°çš„ç¬¬ä¸€ä¸ªå…³é”®è¯ï¼Œå¦‚æœæ²¡æœ‰åŒ¹é…åˆ™è¿”å› Noneã€‚
    (ä¸åŒºåˆ†å¤§å°å†™)
    """
    if not keywords:
        return None
        
    # ç»„åˆæ ‡é¢˜å’Œæ‘˜è¦è¿›è¡Œæœç´¢
    text_to_search = (entry.title + " " + entry.summary).lower()
    
    for kw in keywords:
        if kw.lower() in text_to_search:
            return kw # è¿”å›åŒ¹é…åˆ°çš„è¯
            
    return None

# --- 2. è¯¦ç»†ç‰ˆæ¶ˆæ¯æ ¼å¼ (å«æ‘˜è¦) ---
def format_entry_detailed(entry, max_length=800, matched_keyword=None):
    title = html.escape(entry.title.replace('\n', ' ').strip())
    authors = html.escape(entry.author)
    
    # æ‘˜è¦æ¸…æ´—
    raw_summary = entry.summary.replace('\n', ' ')
    clean_text = re.sub(r'<[^>]+>', '', raw_summary).strip()
    
    pattern = r'arXiv:([^\s]+)\s+Announce Type:\s+(.*?)\s+Abstract:\s+(.*)'
    match = re.search(pattern, clean_text, re.IGNORECASE)
    
    paper_type = "Unknown"
    abstract_text = clean_text
    
    if match:
        paper_type = match.group(2).strip()
        abstract_text = match.group(3).strip()
    
    tags = get_paper_tags(entry)
    tags_str = ", ".join(tags)

    if len(abstract_text) > max_length:
        abstract_text = abstract_text[:max_length] + "..."
    
    summary = html.escape(abstract_text)
    
    abs_link = entry.link
    pdf_link = abs_link.replace("/abs/", "/pdf/") + ".pdf"
    
    type_emoji = "ğŸ†•" if "new" in paper_type.lower() else "ğŸ”„"
    type_label = f"<code>[{paper_type.upper()}]</code>"
    tags_label = f"ğŸ· <code>{tags_str}</code>"
    
    # [æ–°å¢] å¦‚æœæ˜¯å› ä¸ºå…³é”®è¯å‡çº§çš„ï¼Œæ˜¾ç¤ºç‰¹æ®Šæ ‡ç­¾
    keyword_label = ""
    if matched_keyword:
        keyword_label = f"\nğŸ¯ <b>Keyword Match:</b> <code>{matched_keyword}</code>"
    
    msg = (
        f"<b>ğŸ“„ {title}</b>\n"
        f"{type_emoji} {type_label} | {tags_label}{keyword_label}\n\n"
        f"<b>ğŸ‘¥ Authors:</b> {authors}\n\n"
        f"<b>ğŸ“ Abstract:</b>\n{summary}\n\n"
        f"ğŸ”— <a href='{pdf_link}'>PDF Download</a> | <a href='{abs_link}'>Abs Page</a>"
    )
    return msg

# --- 3. æ‰¹é‡å‘é€ Digest ---
def send_digest_messages(simple_buffer):
    if not simple_buffer:
        return

    all_lines = []
    header = "<b>ğŸ—ï¸ Daily Digest (Other Categories)</b>\n"
    all_lines.append(header)

    for category, entries in simple_buffer.items():
        cat_header = f"\n<b>ğŸ“‚ {category}</b>\n"
        all_lines.append(cat_header)

        for entry in entries:
            title = html.escape(entry.title.replace('\n', ' ').strip())
            authors_full = html.escape(entry.author)
            pdf_link = entry.link.replace("/abs/", "/pdf/") + ".pdf"
            
            line = f"ğŸ”¹ <a href='{pdf_link}'>{title}</a>\n    <i>{authors_full}</i>\n"
            all_lines.append(line)

    MAX_LENGTH = 4000
    current_message = ""
    
    for line in all_lines:
        if len(current_message) + len(line) > MAX_LENGTH:
            send_telegram_message(current_message)
            time.sleep(1)
            current_message = line
        else:
            current_message += line
            
    if current_message:
        send_telegram_message(current_message)

def main():
    config = load_config()
    
    detailed_categories = config.get("detailed_categories", [])
    digest_categories = config.get("digest_categories", [])
    keywords = config.get("keywords", [])
    summary_length = config.get("summary_length", 800)
    
    all_categories = detailed_categories + digest_categories
    
    if not all_categories:
        print("æç¤º: detailed_categories å’Œ digest_categories å‡ä¸ºç©ºï¼Œæ— ä»»åŠ¡ã€‚")
        sys.exit(0)

    utc_now = datetime.now(pytz.utc)

    rss_url = get_rss_url(all_categories)
    print(f"æ­£åœ¨è·å– RSS: {len(all_categories)} ä¸ªåˆ†ç±»...")
    feed = feedparser.parse(rss_url)
    print(f"è·å–åˆ° {len(feed.entries)} ç¯‡æ–‡ç« ")

    count = 0
    detailed_count = 0
    simple_buffer = defaultdict(list)

    for entry in feed.entries:
        if not is_today(entry.published_parsed):
            continue

        paper_tags = get_paper_tags(entry)
         
        # 1. å…ˆæ£€æŸ¥æ˜¯å¦å±äºæ ¸å¿ƒç²¾è¯»åˆ†ç±» (Detailed)
        # å¦‚æœå‘½ä¸­ï¼Œç›´æ¥å‘é€ï¼Œä¸åšå…³é”®è¯æ£€æŸ¥ (èŠ‚çœæ—¶é—´)
        if any(tag in detailed_categories for tag in paper_tags):
            print(f"[{count+1}] å‘é€ (è¯¦ç»† - æ ¸å¿ƒ): {entry.title[:30]}...")
            # æ³¨æ„ï¼šè¿™é‡Œ matched_keyword ä¼  Noneï¼Œå› ä¸ºæˆ‘ä»¬ä¸ºäº†çœæ—¶é—´æ²¡å»æŸ¥
            msg = format_entry_detailed(entry, max_length=summary_length, matched_keyword=None)
            send_telegram_message(msg)
            detailed_count += 1
            time.sleep(1)
            
        # 2. å¦‚æœä¸å±äºç²¾è¯»ï¼Œå†æ£€æŸ¥æ˜¯å¦å±äºæ³›è¯»åˆ†ç±» (Digest)
        elif any(tag in digest_categories for tag in paper_tags):
            
            # åªæœ‰åœ¨å®ƒæ˜¯ Digest å€™é€‰æ—¶ï¼Œæ‰å»è·‘å…³é”®è¯æ£€æŸ¥ (Lazy Check)
            matched_keyword = check_keywords(entry, keywords)
            
            if matched_keyword:
                # å‘½ä¸­å…³é”®è¯ -> å‡çº§ä¸ºè¯¦ç»†å‘é€
                print(f"[{count+1}] å‘é€ (è¯¦ç»† - å…³é”®è¯å‡çº§: {matched_keyword}): {entry.title[:30]}...")
                msg = format_entry_detailed(entry, max_length=summary_length, matched_keyword=matched_keyword)
                send_telegram_message(msg)
                detailed_count += 1
                time.sleep(1)
            else:
                # æ²¡å‘½ä¸­å…³é”®è¯ -> ä¹Ÿå°±æ˜¯æ™®é€šçš„ Digest
                target_cat = next((tag for tag in paper_tags if tag in digest_categories), "Others")
                print(f"[{count+1}] ç¼“å­˜ (Digest -> {target_cat}): {entry.title[:30]}...")
                simple_buffer[target_cat].append(entry)
        
        # 3. æ—¢ä¸åœ¨ Detailed ä¹Ÿä¸åœ¨ Digest (å¯èƒ½æ˜¯ RSS å¸¦æ¥çš„æ— å…³äº¤å‰å¼•ç”¨) -> è·³è¿‡
        else:
            continue

        count += 1

    # å¾ªç¯ç»“æŸåå‘é€ Digest
    if simple_buffer:
        print(f"æ­£åœ¨æ„å»ºå¹¶å‘é€ Digest...")
        send_digest_messages(simple_buffer)

    print(f"ä»»åŠ¡å®Œæˆã€‚å…±å¤„ç† {count} ç¯‡ (è¯¦ç»†: {detailed_count}, ç®€æŠ¥: {count - detailed_count})")


if __name__ == "__main__":
    main()